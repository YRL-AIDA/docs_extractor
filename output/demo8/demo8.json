{
    "title": "CNN Is All You Need ",
    "abstract": "The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learning; recently it has also challenged Recurrent Neural Networks (RNNs) with long short-term memory cells (LSTM) in sequence-tosequence learning, since the computations involved in CNN are easily parallelizable whereas those involved in RNN are mostly sequential, leading to a performance bottleneck. However, unlike RNN, the native CNN lacks the history sensitivity required for sequence transformation; therefore enhancing the sequential order awareness, or position-sensitivity, becomes the key to make CNN the general deep learning model. In this work we introduce an extended CNN model with strengthen position-sensitivity, called PoseNet. A notable feature of PoseNet is the asymmetric treatment of position information in the encoder and the decoder. Experiments shows that PoseNet allows us to improve the accuracy of CNN based sequence-to-sequence learning significantly, achieving around 33-36 BLEU scores on the WMT 2014 English-to-German translation task, and around 44-46 BLEU scores on the English-to-French translation task. ",
    "keywords": null,
    "language": "en",
    "sections": [
        {
            "title": "Abstract",
            "text": "The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learning; recently it has also challenged Recurrent Neural Networks (RNNs) with long short-term memory cells (LSTM) in sequence-tosequence learning, since the computations involved in CNN are easily parallelizable whereas those involved in RNN are mostly sequential, leading to a performance bottleneck. However, unlike RNN, the native CNN lacks the history sensitivity required for sequence transformation; therefore enhancing the sequential order awareness, or position-sensitivity, becomes the key to make CNN the general deep learning model. In this work we introduce an extended CNN model with strengthen position-sensitivity, called PoseNet. A notable feature of PoseNet is the asymmetric treatment of position information in the encoder and the decoder. Experiments shows that PoseNet allows us to improve the accuracy of CNN based sequence-to-sequence learning significantly, achieving around 33-36 BLEU scores on the WMT 2014 English-to-German translation task, and around 44-46 BLEU scores on the English-to-French translation task. ",
            "type": "abstract",
            "page_start": 0,
            "page_end": 0
        },
        {
            "title": "1 Introduction ",
            "text": "CNNs have been successfully used in audio, image and text classification, analysis and generation [12,17,18], whereas the RNNs with LSTM cells [5,6] have been widely adopted for solving sequence transduction problems such as language modeling and machine translation [19,3,5]. The RNN models typically align the element positions of the input and output sequences to steps in computation time for generating the sequenced hidden states, with each depending on the current element and the previous hidden state. Such operations are inherently sequential which precludes parallelization and becomes the performance bottleneck. This situation has motivated researchers to extend the easily parallelizable CNN models for more efficient sequence-to-sequence mapping. Once such efforts can deliver satisfactory quality, the usage of CNN in deep learning would be significantly broadened. \nCompared to the history-sensitive recurrent models, in sequence-to-sequence learning, convolution models provide the means for efficient non-local referencing across time steps without fully sequential processing, allowing the computations over the whole sequence to be concurrent rather than one element at a time, and further maximizing GPU’s capability for orders of magnitude performance gain. \nConvolution is generally conducted by the matrix operations on batches of records. To capture the element-wise sequential context in the record-level processing, several mechanisms have been proposed recently such as position encoding, kernel dilation, attention, etc, in ConvS2S [7], Xception [6], ByteNet [12], WaveNet [17] and SliceNet [11]. To reduce the number of parameters, and hence the computation cost, some additional optimizations such as depth-wise convolution [11] and multi-head attention [19], have been introduced, which is commonly characterized by dividing \nelements along the channel dimension, parallelizing the sub-processing and then concatenating the partial results. \nIn this work we focus on further enhancing the position-sensitivity in the CNN based sequence-tosequence learning framework, we explore how to distinguish various operational phases to apply the right mechanisms in the right contexts, for maximizing the expected benefits and minimizing the unwanted side-effects. Specifically we encapsulate a group of neurons for convolutions in a convolution box whose activity vector represents the instantiation parameters – a similar treatment found in [7,11], as the basic building blocks of our architecture. When using such convolution boxes in the encoder and the decoder, we customize their internal structures depending on where they are used, with different sub-layers for dealing with the sequential position information in the corresponding context. A notable feature of our approach is the asymmetric treatment of position information in the encoder and the decoder. According to the difference of how sequential information is presented and used in the encoding and the decoding processes, we repeat positionencoding (or timing signal) along multiple layers only in the encoder; apply dilation convolution for encoding and regular convolutions for decoding, and organize self-attention, cross-attention, position-wise feed-forward with customized residual links [8], selectively in both the encoder and the decoder. \nWe implement the above context-sensitive position-sensitive mechanisms in an extended CNN model, PoseNet, for improving the accuracy of convolution based sequence-to-sequence learning. Experiments show that using PoseNet allows us to achieve around 33-36 approximate BLEU scores on the WMT 2014 English-to-German translation task with batch-size 2048 and 250K-500K training steps, as well as to get superior performance in English-to-French translation, achieving 44-46 approximate BLEU scores with batch-size 2048 and 1000K training steps. \nIn the similar way we also enhanced the accuracy of the “attention-only” approach [19] by an extended model that actually outperforms the PoseNet described here in the same tasks; however, as we believe that in certain areas such as image recognition, the CNN can provide higher generality, in this work we focus on exploring the universal use of the CNN, particularly in sequence-to-sequence learning, and have the other efforts reported separately. \n",
            "type": null,
            "page_start": 0,
            "page_end": 1
        },
        {
            "title": "2 History-Sensitivity and Position-Sensitivity ",
            "text": "Sequence-to-sequence learning is typically based on the encoder-decoder architectures [10,11,13,19,21] where the encoder processes an input sequence $x = ( x _ { I } , \\ldots , x _ { n } )$ of $n$ elements and returns the internal representations $h = ( h _ { I } . \\ . \\ . \\ , \\ h _ { n } )$ , and the decoder takes $h$ to generate the output sequence $y = ( y _ { I } , \\ldots , y _ { m } )$ left to right, one element at a time. \nIn the RNN based sequence-to-sequence learning [5,9], the above $h$ is computed sequentially and kept as the revisable, long or short, history. To generate output $y _ { i + I }$ , the decoder computes a new hidden state $h _ { i + I }$ based on the previous state $h _ { i } ,$ the previous output $y _ { i } ,$ , as well as a conditional input $c _ { i }$ derived from the encoder output $h$ . The models without attention consider only the final encoder state $h _ { n } .$ either by ignoring $c _ { i }$ or by setting $c _ { i } = h _ { n }$ for all position i. Architectures with attention compute $c _ { i }$ as a weighted sum of $( h _ { I } . . . . , h _ { n } )$ at each time-step as the corresponding attention scores, focusing on different parts of the input sequence. Attention scores are computed by comparing each encoder state to a combination of the previous decoder state and the last prediction element; the result is normalized to be a distribution over input elements [1]. \nThe CNN based sequence-to-sequence transformation follows this high-level scenario in general, but with the intermediate encoder and decoder representations calculated by convolutions in parallel for all input and output positions. Usually both encoder and decoder networks are stacked with a kind of convolution layer that computes intermediate representations based on a fixed number of input elements. Stacking several layers on top of each other increases the range of input elements represented. Further, convolutions are often followed by non-linearities, allowing the networks to focus on wider input field. Padding is employed in both encoding and decoding to \nensure the match of the input length, and to ensure that at a step, no future information is available to the decoder. \nIn summary, in sequence-to-sequence learning, RNN relies on history sensitive sequential computation, but CNN relies on position-sensitive parallel computation. \n",
            "type": null,
            "page_start": 1,
            "page_end": 2
        },
        {
            "title": "3 Strengthen Position-Sensitivity in Convolutional Sequence Learning ",
            "text": "To optimize CNN for sequence-to-sequence learning, let us first understand how the concept of \"sequence\" is caught in a CNN framework. \nIn sequence transduction tasks, the sense of “sequence” is represented by long-range dependencies. One important factor affecting the ability to learn such dependencies is the length of the paths between any combination of positions in the input and output sequences, which the signals have to traverse forward and backward in the network; the shorter these paths the easier to learn longrange dependencies. \nFrom this point of view convolution provides a key advantage for sequence-to-sequence learning, since a multi-layer convolution stack can create multi-level representations over the input sequence where nearby input elements interact at lower levels and distant elements interact at higher levels [7]. This way, the hierarchical structure modeled by CNN provides shorter paths compared to the chain structure modeled by RNN. In the CNN with kernels of width $k _ { - }$ , a feature representation capturing relationships within a window of n elements (such as words) can be accessed by applying only $O ( n / k )$ convolution operation, compared to a linear number $O ( n )$ in an RNN. \nFurther, the CNN based sequence mapping can be naturally parallelized since inputs are fed through a constant number of kernels and non-linearities, whereas the number of operations and non-linearities applied in an RNN varies from position to position. \n3.1 How Position Relationships Captured \nConvolution, like other deep learning operations, is essentially a tensor-to-tensor mapping. In sequence transformations, the positional relationships between the elements are handled along with the matrix manipulation of tensors. \n",
            "type": null,
            "page_start": 2,
            "page_end": 2
        },
        {
            "title": "Position Encoding ",
            "text": "In sequence-to-sequence learning, an input or target record consists of a sequence of elements along time-steps. A common way for a deep learning model to make use of the order of the elements is to inject some information about the relative or absolute position of the elements in the sequence [7]. Typically the input elements $x = ( x _ { I } , \\ . \\ . \\ . \\ , x _ { m } )$ are first embedded in distributional space $w = ( w _ { I } , \\dots , w _ { m } )$ with depth d. To equip the model with a sense of the absolute position of input elements, $p = ( p _ { I } , \\ldots , p _ { m } )$ is encoded, forming the combined input element representation $e$ $= ( w _ { I } + p _ { I } , \\ . \\ . \\ . \\ , \\ w _ { m } + p _ { m } )$ $w _ { m } { + p _ { m } } )$ . The target elements are processed similarly in their encoding phase. Note that the positional encodings form a tensor with the same depth $d$ as the input embeddings, so that the two can be summed. Usually positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. \nThere exist various choices of positional encodings; the following position encoding function $f _ { p e }$ uses sine and cosine functions of different frequencies: \n$$\nf _ {p e} (p o s, 2 i) = \\sin (p o s / 1 0 0 0 0 ^ {2 i / d})\n$$\n$$\nf _ {p e} (p o s, 2 i + 1) = c o s (p o s / 1 0 0 0 0 ^ {2 i / d})\n$$\nwhere pos is the position, $i$ the dimension and $d$ the depth. \nSince position encoding gives the model a sense of order of the input or target sequence it is currently dealing with, we explored the way to repeat it at the appropriate layers of the model graph to strengthen the position sensitivity at multiple layers, but without introducing unexpected noises. \n",
            "type": null,
            "page_start": 2,
            "page_end": 3
        },
        {
            "title": "Position-wise Feed-Forward Networks (ffn) ",
            "text": "Encoding/decoding in sequence-to-sequence mapping essentially consists in determining the positional correlation between the input/target pairs, this is why the position-wise feed-forward networks $( f \\mathscr { f } n )$ come to the picture. An ffn is similar to a convolution with kernel size 1 that is applied to each position separately and identically. An $n$ -layer $f \\mathscr { f } n ^ { n }$ performs $n$ linear transformations with a ReLU activation in between, which can be intuitively described as below. \n$$\n\\begin{array}{l} f f n ^ {l} (x) = x W _ {l} + b _ {l} \\\\ f f n ^ {2} (x) = \\max  (0, x W _ {1} + b _ {1}) W _ {2} + b _ {2} \\\\ f f n ^ {n} (x) = \\max  (0, f f n ^ {n - 1} (x)) W _ {n} + b _ {n} \\\\ \\end{array}\n$$\nAlthough such linear transformations are the same across different positions, it is position awareness due to that the transformations use different parameters from layer to layer. Therefore adding $f \\mathscr { f } n$ to the appropriate points in the model graph provides a way to enhance position sensitivity. \n",
            "type": null,
            "page_start": 3,
            "page_end": 3
        },
        {
            "title": "Filter Dilation ",
            "text": "Filter dilation is a mechanism for correlating distant elements in convolutional sequence-tosequence autoregressive approach. Essentially, filter dilation increases the receptive field of the convolution operation by enlarging the spatial extent from which feature information can be gathered. In the other words, the dilated convolution operators can use the same filter at different ranges using different dilation factors. \nCompared to using larger convolution windows, using filter dilation has the pros of lower computation cost, and the cons of unequal convolutional coverage of the input space. Our observation indicates that for boosting position awareness, the dilation mechanism has stronger effect in encoder than in decoder. \n",
            "type": null,
            "page_start": 3,
            "page_end": 3
        },
        {
            "title": "Cross-Attention and Self-Attention ",
            "text": "The simple inner-product attention correlates two tensors position-wise. Given two tensors $S [ n , d ]$ and $\\pi m , d ]$ , where $d$ stands for depth, according to [11,19], the attention mechanism computes the feature vector similarities at each position and re-scales according to the depth: \n$$\n\\operatorname {a t t e n t i o n} (S, T) = 1 / \\sqrt {d}. \\operatorname {S o f t m a x} \\left(T \\cdot S ^ {T}\\right) \\cdot S\n$$\nwhere $S$ and $T$ can be two different tensors or the same tensor; we refer to the attention in the former case cross-attention, and in the latter case self-attention. \nCross-attention is often used in the \"encoder-decoder attention\" layers, where $T$ comes from the previous decoder layer, and $S$ from the encoded input, i.e. output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. Self-attention is used in input encoding and target encoding where S and $T$ come from the same place – the output of the previous layer in the encoder or in the decoder. A self-attention layer in the encoder allows each position to attend to all positions in the previous layer of the encoder; similarly, a self attention layer in the decoder allow each position to attend to all positions in the decoder up to and \nincluding that position. To prevent leftward information flow in the decoder to preserve the autoregressive property [11,12,17], masking out the values in the input of the softmax which correspond to illegal positions, is necessary. \nOne way to strengthen the positional relationship in convolution based sequence-to-sequence learning is to apply the cross-attention and self-attention multiple times. We have experienced this with certain accuracy gain. \n3.2 Design Consideration \nIn order to capture richer sequential position information and position-wise relationships between inputs and targets for improved accuracy of CNN based sequence-to-sequence learning, we follow these design considerations: the impact of position-sensitivity on the accuracy of sequence-tosequence learning varies in the encoding process and decoding process; as a result, context specific convolution boxes are needed for applying the above position-sensitive mechanisms. \nIntuitively, when inputs are encoded, the sequences dealt with are to be completely populated thus sensitive to enhanced (e.g. repeated) position encoding. However, during decoding, in the partially generated targets with pads as space-fillers, some of the above mechanisms, such as repeated position encoding, would be ineffective or even noisy. The context sensitivity of other mechanisms for capturing position information can be explained similarly. \n",
            "type": null,
            "page_start": 3,
            "page_end": 4
        },
        {
            "title": "4 PoseNet Architecture ",
            "text": "Our PoseNet architecture is built using the tensor2tensor library [23] and extending the Slicenet [11] model. We follow the conceptual encoder-decoder structure [10,11,13,19,21], where the encoder maps an input sequence representations $( x _ { I } , . . . , x _ { n } )$ to a sequence of continuous hidden representations $h = ( h _ { I } , . . . , h _ { n } )$ ; from there, the decoder then generates an output sequence $( \\boldsymbol { y } _ { I } , . . . ,$ $y _ { m } )$ one element at a time. At each step the model is auto-regressive, consuming the previously generated elements as additional input when generating the next. Our architecture realizes this overall encoder-decoder structure using stacked convolution boxes and point-wise, fully connected layers, shown in the left and right halves of Figure 1, respectively. However, these convolution boxes are customized differently for the encoder and the decoder. \n",
            "type": null,
            "page_start": 4,
            "page_end": 4
        },
        {
            "title": "Encoder ",
            "text": "The encoder is composed of a stack of $\\mathrm { L } = 6$ layers. Each layer has two convolution boxes with residual links and a simple, position-wise fully connected feed-forward net. For each sub-layer with function $f ,$ we employ a residual connection followed by layer normalization [1], i.e. produce norm $( x + f ( x ) )$ . In addition, each layer begins with a position encoding, and ends with a positionwise feed-forward net, both of these allow us to strengthen the position-sensitivity of our model as they are repeated in all the $\\mathrm { ~ L ~ } = \\mathrm { ~ 6 ~ }$ layers. We also choose to invoke the dilation convolutions provided in the tensor2tensor library for encoding, to position-wise correlate distant elements. It is worth noting that we repeat position-encoding and use dilation convolution only for encoding, but not for decoding, for the reasons explained. \n",
            "type": null,
            "page_start": 4,
            "page_end": 4
        },
        {
            "title": "Decoder ",
            "text": "Our model follows the convolutional autoregressive structure introduced in Slicenet[11], ByteNet [12], WaveNet [17] and PixelCNN [18]. Inputs are embedded and encoded before being fed into a decoder that auto-regressively generates each element of the output. At every step, the autoregressive decoder produces a new output prediction given the encoded inputs and the \nencoding of the existing predicted outputs. The outputs. The basic modules are the convolution boxion boxes stacked, and the attention modules for for the decoder to get information from the encoder. As we reuse theget information encoder. As corresponding functions from the tensorflow library and the tensor2tensor library, we skip the skip their details here. \nWe use a stack of $\\mathrm { ~ L ~ } = 5$ decoder layers. T  layers. The cross-attention, that allows each decoding step tocoding to attend the encoded-inputs, is repeatedly applied in each layer. inputs, applied layer. In the same way as the encoder, as encoder, each decoder layer ends with a position with a position-wise feed-forward net. Each decoder layer also has twoforward net. convolution boxes with residual links and residual a simple, position-wise fully connected feed wise feed-forward network. Masking is used to prevent positions from attending to subsequent positions. The maskasking used prevent attending subsequent asking and the offset of output embeddings ensure set  that the prediction for each position to depend only onto only the known outputs at positions less than that position. \nIn summary, in capturing the sequential position information and the position-wise relationships between inputs and targets, the PoseNet treats the convolution boxes for the encoder and those for the decoder differently; in the other words, the encoder and the decoder employ context specific convolution boxes which involve the position-sensitive mechanisms in different ways. In the encoder stack, the position encoding and the position-wise feed-forward net are applied to the beginning and the ending of each layer, repeatedly, and the convolutions are dilated to be more sensitivity to the distant positions. In the decoder stacks, the cross-attention is used at each layer repeatedly, but the position encoding is optionally applied only once. \nWe chose not to use dilated convolution for decoder as in the step-by-step decoding process, outputs and pads shift thus less sensitive to dilation. For the same reason, we do not apply, or optionally apply only once without repeating in every layer, the position encoding, because in this case it has less effect or even adds noise. Please also note that in each layer the cross-attention is place before the convolution boxes. \nFor enhanced parallelism and/or reduced parameters, the mechanisms of multi-head attention [19] and depth-wise convolution [11] are introduced, which are in common characterized by splitting the channels of an input into several non-overlapping segments, performing a regular attention or spatial convolution over each segment independently and then concatenating the resulting feature maps along the channel axis. The details of these mechanisms can be found in [19], [11] and the tensorflow API specifications. In this work we take advantage of them. \n",
            "type": null,
            "page_start": 4,
            "page_end": 6
        },
        {
            "title": "5 Experiments ",
            "text": "We carry out our experiments on x86_64 GNU/Linux with 4.4.0-96-generic #119-Ubuntu with 8G memory, using one NVIDIA GeForce GTX 1070 with CUDA V8.0.61, python3.5.2, tensorflow-1.3.0, tensor2tensor-1.2.2. In this study we focus on the follows issues: \nHow to improve the position sensitivity in the CNN-based, non-recurrent sequence-tosequence learning for enhanced accuracy, and \nHow to apply the position-sensitivity strengthen mechanisms differently in the encoder and in the decoder. \nOur model is trained and evaluated for the WMT English-to-German (EN-DE) and English-to-French (EN-FR) translation tasks using the benchmark data translate_ende_wmt $3 2 k$ and translate_enfr_wmt32k. The sizes of the sample data, measured on disk, are about 689M for EN-DE and 11G for EN-FR. All of our experiments are implemented using the tensorflow framework and the tensor2tensor library [23]. We also leveraged the convolution and attention mechanisms described in the related work [4, 11, 12,17, 19]. \n",
            "type": null,
            "page_start": 6,
            "page_end": 6
        },
        {
            "title": "Our experiments reveal that ",
            "text": "the convolution based sequence-to-sequence learning can be benefited by the combination of several position sensitivity strengthen mechanisms such as repeatedly imposing timing signal, selectively apply dilation, appropriate mix parameterized and convolution-based self-attention, etc; \ncertain position sensitivity strengthen mechanisms are more effective for accuracy enhancement if applied to encoding, including input encoding and target encoding, rather than to decoding. \nThe intermediate training states are checkpointed by the tensorflow framework; along with each checkpoint (e.g. 2000 steps) the corresponding evaluation results are logged. We attached the partial evaluation results at the last 10 checkpoints on training the EN-DE translation model in Appendix 8.1; and on training the EN-FR translation model in Appendix 8.2. The average of these BLEU scores are approximately consistent with those measured at the 10 additional checkpoints \nafter the given train steps. We did not average the BLEU scores measured at the previous checkpoints where the model is still under-trained. \nUsing our model, with batch-size 2048 and using 1 GPU, for the EN-DE translation task we achieved 33-36 approximate BLEU scores in 250K-500K training steps; for the EN-FR translation task we achieved 44-46 approximate BLEU scores in 1000K training steps (on a multiple GPU machine we see slightly higher accuracy but lower BLEU scores). \nTo justify the significance of our approach, we have the BLEU scores on the EN-DE and the EN-FR translations compared with the related work (given in [11,19]), as listed in table 1 below. \n",
            "type": null,
            "page_start": 6,
            "page_end": 7
        },
        {
            "title": "6 Conclusions ",
            "text": "The goal of providing a generalized and efficient deep-learning framework has motivated us to explore the possibility of using CNN as the universal building blocks. As CNN already has the superior track-record in audio, image and text learning, we particularly focus on its potential in sequence-to-sequence learning. Compared to the computations involved in RNN, the computations involved in CNN are easily parallelizable, but less history sensitive. Therefore enhancing the sequential order awareness, or position-sensitivity, is the key for CNN to support sequence transformation. In this work we introduce a CNN architecture, PoseNet, which is characterized by applying the position-sensitive mechanisms - position encoding (or timing signal), self-attention and cross-attention, dilation convolution, position-wise feed-forward net, residual net, etc, selectively in the encoder and the decoder for maximizing their effects. A notable feature of PoseNet is the asymmetric treatment of position information in the encoder and the decoder. For this we turn the common convolution boxes into specific ones depending on where they are used, with different sub-layers for capturing the context specific sequence oriented information. Experiments show that with strengthen position-sensitivity, PoseNet is capable of improving the accuracy of convolutional sequence-to-sequence learning - achieving around 33-36 approximate BLEU scores, and 44-46 approximate BLEU scores, on the WMT 2014 English-to-German and English-to-French translation tasks respectively. \n",
            "type": null,
            "page_start": 7,
            "page_end": 7
        },
        {
            "title": "7 References ",
            "text": "",
            "type": null,
            "page_start": 8,
            "page_end": 8
        }
    ],
    "references": [],
    "figures": [
        {
            "id": "Figure 1",
            "type": "image",
            "caption": [
                "Figure 1: The encoder (left) and decoder (right) stacks and stacks: "
            ],
            "img_path": [
                "images/f8b80ee8a1d800f5f7bd20617cd7703bd66c8dd461ff839554d49707164a42bb.jpg"
            ],
            "page": 5
        },
        {
            "id": "Table 1",
            "type": "table",
            "caption": [
                "WMT 2014 En-De Translation Task ",
                "Table 1: Performance of our models in EN-DE and EN-FR translation tasks with benchmark data translate_ende_wmt $3 2 k$ and translate_enfr_wmt $3 2 k$ compared to the latest published results; the “avg BLEU” averages the scores measured at the last 10 checkpoints "
            ],
            "table_body": "<table><tr><td>Model</td><td>EN-DE</td><td>BLEU</td><td>EN-FR</td><td>BLEU</td></tr><tr><td>Bytenet [12]</td><td>23.75</td><td></td><td></td><td></td></tr><tr><td>GNMT + RL [20]</td><td>24.6</td><td></td><td>39.92</td><td></td></tr><tr><td>ConvS2S [7]</td><td>25.16</td><td></td><td>40.46</td><td></td></tr><tr><td>MoE [14]</td><td>26.03</td><td></td><td>40.56</td><td></td></tr><tr><td>GNMT + RL Ensemble [22]</td><td>26.30</td><td></td><td>41.16</td><td></td></tr><tr><td>ConvS2S Ensemble [7]</td><td>26.36</td><td></td><td>41.29</td><td></td></tr><tr><td>Transformer (base) [19]</td><td>27.3</td><td></td><td>38.1</td><td></td></tr><tr><td>Transformer (big) [19]</td><td>28.4</td><td></td><td>41.0</td><td></td></tr><tr><td>SliceNet (Full, 2048) [11]</td><td>25.5</td><td></td><td></td><td></td></tr><tr><td>SliceNet (Super 2/3, 3072) [11]</td><td>26.1</td><td></td><td></td><td></td></tr><tr><td>PoseNet (2048)</td><td colspan=\"2\">33-36 (avg 34.92)</td><td colspan=\"2\">44-46 (avg 45.54)</td></tr><tr><td></td><td colspan=\"2\">(250K-500K steps)</td><td colspan=\"2\">(1000K steps)</td></tr></table>",
            "img_path": "output/images/2b9d189f7c029c0e47649251d3e0fa9748ff841f60fce539133130f73109d24a.jpg",
            "page": 7
        }
    ]
}